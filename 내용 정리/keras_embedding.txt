%배경
2016년 rossman store sales 경진대회에서 Cheng Gou와 Felix Berkhahn가 3등을 했다.
여기서 그들이 분석에 활용했던 방식이 embedding이었다. 당시 꽤 좋은 성능을 보였고 지금도 활용되는 embedding은 무엇인가

%의미
단순 의미는 범주형 자료를 연속형 벡터 형태로 변환시키는 것을 의미한다.
단어를 처리함에 있어서 one-hot encoding 방식은 단어 수만큼 벡터의 길이가 늘어나는 단점이 있다.
이를 해결하기 위해 더 조밀한 차원에(사용자가 설정한) 단어를 벡터로 표현하는 것을 단어 임베딩이라 한다.

%사용 이유
1)차원을 축소해줄 수 있다.(one-hot encoding에서는 (n-1)개의 차원이 필요한데 임베딩에서는 2, 3차원으로도 표현할 수 있다.)
2)범주형 자료를 연속형으로 표현할 수 있다.
3)의미를 도출하기에 용이하다.

%기본적인 사용
model.add(Embedding(input_dim, output_dim, input_length=?))

input_dim : 단어 사전의 크기를 의미합니다.
output_dim : 단어를 인코딩한 후 나오는 벡터 크기입니다.
input_length : 단어의 수, 즉 문장 길이를 나타냅니다. 임베딩 레이어의 출력 크기는 output_dim * input_length가 됩니다. 임베딩 레이어 이후에 Flatten 레이어가 온다면 반드시 지정해줘야 합니다.


@왜 word2vec를 쓰는가
NLP(Natural Language Processing, 자연어 처리)는 ‘컴퓨터가 인간이 사용하는 언어를 이해하고, 분석할 수 있게 하는 분야’ 를 총칭하는 말이다. 그러나 얼핏 생각했을 때 컴퓨터가 이를 이해할 수 있도록 변환하는 것은 참 어려운 일일 것이다.
컴퓨터에게 ‘사과’와 ‘초콜릿’ 이라는 두 단어를 보여준다고 컴퓨터가 두 단어의 개념적인 차이를 이해할 수 있을까? 컴퓨터는 단순히 두 단어를 유니코드의 집합으로만 생각할 것이다.
기본적으로 컴퓨터가 어떤 단어에 대해 인지할 수 있게 하기 위해서는 수치적인 방식으로 단어를 표현할 수 있어야 한다. 그러나 앞서 말했듯이, 수치화를 통해 단어의 개념적 차이를 나타내기가 근본적으로 힘들었다. 이 한계를 극복하기 전의 NLP는 ‘one-hot encoding’ 방식을 많이 사용했다.
예를 들어, 내가 어떤 단어들이 있는지에 대한 단어 n개짜리 ‘사전’ (Dictionary)이 있다고 해보자. 이 때, 어떤 단어를 표현하기 위해서 길이 n짜리 벡터를 하나 만들고, 그 단어가 해당되는 자리에 1을 넣고 나머지 자리들에는 0을 넣는 것이다.  사전이 [감자, 딸기, 사과, 수박] 이라면 사과를 표현하는 벡터는 [0, 0, 1, 0] 이 되는 식이다.
이 방식은 당시에는 나름 좋은 성능을 내었고, 지금까지도 사용하는 사람들이 있지만 컴퓨터 자체가 ‘단어가 본질적으로 다른 단어와 어떤 차이점을 가지는 지 이해할 수가 없다’ 는 아주 큰 단점이 존재한다.

이러한 단점을 존재하기 위해 연구자들은 단어 자체가 가지는 의미 자체를 다차원 공간에서 ‘벡터화’ 하는 방식을 고안하게 되었다. 단어의 의미 자체를 벡터화할 수 있게 된다면, 기본적으로 이것을 사용해서 할 수 있는 일들이 굉장히 많아진다. 일단 단어들이 실수 공간에 흩어져있다고 생각할 수 있기 때문에 각 단어들 사이의 유사도를 측정할 수가 있고, 여러 개의 단어에 대해 다룰 때도 더해서 평균을 내는 등 수치적으로 쉽게 다룰 수가 있다. 또한 가장 좋은 점은 의미 자체가 벡터로 수치화되어 있기 때문에, 벡터 연산을 통해서 추론을 내릴 수가 있다는 점이다. 예를 들어, ‘한국’에 대한 벡터에서 ‘서울’에 대한 벡터를 빼고, ‘도쿄’ 에 대한 벡터를 넣는다면? 이 벡터 연산 결과를 통해 나온 새로운 벡터와 가장 가까운 단어를 찾으면, 놀랍게도 이 방식으로 ‘일본’ 이라는 결과를 얻을 수가 있다. 이것이 기존 방법과 가장 큰 차이점이자 장점이다.

